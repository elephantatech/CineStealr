services:
  llm:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: cinestealr_llm
    ports:
      - "8080:8080"
    volumes:
      - ./docker/models:/models
    # CPU Optimization for Apple Silicon containers:
    # -c 4096 = context size (LLaVA images need ~3000 tokens)
    # -np 1 = single slot
    # -b 256 = smaller batch size for less memory pressure
    # -t 8 = use 8 CPU threads
    command: -m /models/llava-v1.5-7b-Q4_K.gguf --mmproj /models/mmproj-model-f16.gguf --host 0.0.0.0 --port 8080 -c 4096 -np 1 -b 256 -t 8
    restart: unless-stopped

  backend:
    build: ./backend
    container_name: cinestealr_backend
    depends_on:
      - llm
    ports:
      - "8000:8000"
    environment:
      - LLM_API_URL=http://llm:8080/v1/chat/completions
    volumes:
      - ./backend/uploads:/app/uploads
    restart: unless-stopped

  frontend:
    build: ./frontend
    container_name: cinestealr_frontend
    depends_on:
      - backend
    ports:
      - "5173:5173"
    restart: unless-stopped
